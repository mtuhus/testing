{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Imputation.ipynb",
      "provenance": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPurYPhvNq6fhgiW/5s6nfJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mtuhus/testing/blob/master/Imputation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DhBgCh-x18KR",
        "colab_type": "code",
        "outputId": "93f2d92f-a5f5-486b-f985-c0039f9628e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.test.gpu_device_name()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/device:GPU:0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BE2-35YI3F3L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sowO1F_bBs7D",
        "colab_type": "code",
        "outputId": "b09d494e-92fb-4895-9fcd-bd2490560e2c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "# memory footprint support libraries/code\n",
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isnâ€™t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        " process = psutil.Process(os.getpid())\n",
        " print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        " print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm() "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gputil\n",
            "  Downloading https://files.pythonhosted.org/packages/ed/0e/5c61eedde9f6c87713e89d794f01e378cfd9565847d4576fa627d758c554/GPUtil-1.4.0.tar.gz\n",
            "Building wheels for collected packages: gputil\n",
            "  Building wheel for gputil (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gputil: filename=GPUtil-1.4.0-cp36-none-any.whl size=7413 sha256=1c1796f396045b6fb7c6068195e987791ab2e263584beba7b57b5a17abfe14fe\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/77/07/80562de4bb0786e5ea186911a2c831fdd0018bda69beab71fd\n",
            "Successfully built gputil\n",
            "Installing collected packages: gputil\n",
            "Successfully installed gputil-1.4.0\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 25.7 GB  | Proc size: 1.1 GB\n",
            "GPU RAM Free: 15927MB | Used: 353MB | Util   2% | Total 16280MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-H76gs73Kii",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5NiWS6oc3TyJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E6l7dbZWDUNe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s7mVngYV3ldT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KvFDVkgT3oJ-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "downloaded = drive.CreateFile({'id':'1nSAqJYO5thSWS0gFWDgmTTtsN7DJtQKD'}) # replace the id with id of file you want to access\n",
        "downloaded.GetContentFile('Testing_Set1.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHCLBP5n4ZuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.decomposition import KernelPCA\n",
        "import random as random \n",
        "from random import choice \n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import sys \n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn import preprocessing\n",
        "from sklearn.impute import (SimpleImputer, KNNImputer, IterativeImputer)\n",
        "import random as random \n",
        "from random import choice \n",
        "from sklearn.linear_model import BayesianRidge\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "from sklearn.pipeline import make_pipeline, make_union\n",
        "from sklearn.impute import (SimpleImputer, KNNImputer, IterativeImputer, MissingIndicator)\n",
        "from sklearn import linear_model\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.ensemble import ExtraTreesRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.linear_model import BayesianRidge\n",
        "import random as random \n",
        "from random import choice \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMkTuEE3gGWA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Loading the DataFrame\n",
        "df = pd.read_csv('Testing_Set1.csv', sep = ';') \n",
        "df = df.iloc[1500:5000]\n",
        "df_nan = pd.DataFrame(new_dataframe(df, 0.025))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqIuH6T8CcPL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Randomly removing values from the original dataset\n",
        "def new_dataframe(data, percent):\n",
        "    data = data.to_numpy()\n",
        "    mat = data.copy()\n",
        "    prop = int(mat.size * percent)\n",
        "    mask = random.sample(range(mat.size), prop)\n",
        "    np.put(mat, mask, [np.NaN]*len(mask))\n",
        "\n",
        "    return mat"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D-j9VfqZFpER",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Calculating the Root Mean Square Error and Standard Deviation for the imputed and the original value\n",
        "def mean_error(original, predicted):\n",
        "  original = original.to_numpy()\n",
        "  output = []\n",
        "  \n",
        "  for i in range(len(original)):\n",
        "    x = np.sqrt(((original[i] - np.asscalar(predicted[i]))**2))\n",
        "    output.append(x)\n",
        "\n",
        "  output = np.array(output)\n",
        "  return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Osach0eDB1t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Univariate Imputation Techinques \n",
        "from sklearn.metrics import mean_squared_error\n",
        "data_median = pd.DataFrame(data_nan.fillna(data_nan.median()))\n",
        "data_mean = pd.DataFrame(data_nan.fillna(data_nan.mean()))\n",
        "data_ff = pd.DataFrame(data_nan.fillna(method = 'ffill'))\n",
        "data_bf = pd.DataFrame(data_nan.fillna(method = 'bfill'))\n",
        "data_linear = data_nan.interpolate(method = 'linear', axis = 0).ffill().bfill()\n",
        "data_poly = data_nan.interpolate(method = 'polynomial', order = 5, axis = 0).ffill().bfill()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hspPoPDnI9np",
        "colab_type": "code",
        "outputId": "ce17de5f-cb9e-458e-9e9f-9f0a3f8d89e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "#Calculation of RMSE and STD\n",
        "error_median = mean_error(df['HP Flare'], data_median)\n",
        "error_mean = mean_error(df['HP Flare'], data_mean)\n",
        "error_ff = mean_error(df['HP Flare'], data_ff)\n",
        "error_bf = mean_error(df['HP Flare'], data_bf)\n",
        "error_linear = mean_error(df['HP Flare'], data_linear)\n",
        "error_poly = mean_error(df['HP Flare'], data_poly)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MEAN: 53.45268913934427 1291.2888218929693\n",
            "MEDIAN: 51.86590000000001 1312.773879309453\n",
            "FORWARD FILL: 54.418699999999994 1342.079737690838\n",
            "BACKWARD FILL: 34.0071 806.431091327455\n",
            "LINEAR INTERPOLATION: 43.9171 1061.653456829294\n",
            "POLYNOMIAL INTERPOLATION 55.67648195434812 1216.1048407637127\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YB4lbbZlY58D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imputation for the variabel HP-Flare using several univariate techniques\n",
        "data_nan = pd.DataFrame(df_nan[69])\n",
        "df_1 = pd.DataFrame()\n",
        "df_1['Nan'] = df_nan[69]\n",
        "df_1['Mean'] = pd.DataFrame(data_nan.fillna(data_nan.mean()))\n",
        "df_1['median'] = pd.DataFrame(data_nan.fillna(data_nan.median()))\n",
        "\n",
        "df_1['Foward'] = data_nan.fillna(method = 'ffill')\n",
        "df_1['BackWard'] = data_nan.fillna(method = 'bfill')\n",
        "df_1['Linear'] = data_nan.interpolate(method = 'linear', axis = 0).ffill().bfill()\n",
        "df_1['Poly']= data_nan.interpolate(method = 'polynomial', order = 5, axis = 0).ffill().bfill()\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbzW0mm1857m",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxAVuMaSUvvp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using KNN-Iterative Impuation \n",
        "data_knn = df_nan.copy()\n",
        "imputer_knn = IterativeImputer(estimator = KNeighborsRegressor(n_neighbors = 10), random_state = 0, max_iter = 1000, tol = 0.08)\n",
        "imputer_knn.fit(data_knn)\n",
        "data_knn = imputer_knn.transform(data_knn)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kM48VJz5cSVH",
        "colab_type": "code",
        "outputId": "304c8e49-d004-41fa-f186-bba9c3308b5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:7: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  import sys\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "409.86307558087543"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZSc3xQkYkS5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Using KNN Simple Imputer\n",
        "data_knn1 = df_nan.copy()\n",
        "imputer_knn1 = KNNImputer()\n",
        "imputer_knn1.fit(data_knn1)\n",
        "data_knn1 = imputer_knn1.transform(data_knn1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EWWNKkM2ZAl6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Using ExtraTreeRegressor Iterative Imputer\n",
        "data_exr = df_nan.copy()\n",
        "imputation_exr = IterativeImputer(estimator=ExtraTreesRegressor(n_estimators=10, random_state=0), missing_values=np.nan, sample_posterior=False, \n",
        "                                 max_iter=1000, tol=0.1, \n",
        "                                 n_nearest_features=4, initial_strategy='median')\n",
        "\n",
        "imputation_exr.fit(data_exr)\n",
        "data_exr = imputation_exr.transform(data_exr)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ZPOGejjZKtu",
        "colab_type": "code",
        "outputId": "693e6539-d180-4e0a-ef73-940645a76d5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Iterative Decision Tree Regressor Imputation:\n",
        "data_dt = df_nan.copy()\n",
        "imputer_dt = IterativeImputer(estimator = DecisionTreeRegressor(), missing_values = np.nan, sample_posterior = False, \n",
        "                             max_iter = 1000, tol = 0.1, n_nearest_features = 4, imputation_order = 'ascending')\n",
        "imputer_dt.fit(data_dt)\n",
        "data_dt = imputer_dt.transform(data_dt)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/impute/_iterative.py:638: ConvergenceWarning: [IterativeImputer] Early stopping criterion not reached.\n",
            "  \" reached.\", ConvergenceWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3NerGrSqjK7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Iterative Bayesian Ridge Regressor\n",
        "data_bayesian = df_nan.copy()\n",
        "imputer_br = IterativeImputer(estimator = BayesianRidge(), missing_values = np.nan, random_state = 0, n_nearest_features = 5, sample_posterior = True)\n",
        "imputer_br.fit(data_bayesian)\n",
        "data_bayesian = imputer_br.transform(data_bayesian)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbZUV8R5csJG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#RMSE AND STD FOR ITERATIVE IMPUTER\n",
        "def mean_error_iterative(original, predicted):\n",
        "  original = original.to_numpy()\n",
        "  predicted_df = pd.DataFrame(predicted)\n",
        "  predicted_feature = predicted_df[69]\n",
        "  output = []\n",
        "  \n",
        "  for i in range(len(original)):\n",
        "    x = np.sqrt(((original[i] - np.asscalar(predicted_feature[i]))**2))\n",
        "    output.append(x)\n",
        "\n",
        "  output = np.array(output)\n",
        "  return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1-PQxCJc42g",
        "colab_type": "code",
        "outputId": "21dcd3c7-93c4-4dc0-ed73-7e3091f2d7ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "error_iterativeknn = mean_error_iterative(df['HP Flare'], data_knn)\n",
        "error_simpleknn = mean_error_iterative(df['HP Flare'], data_knn1)\n",
        "error_extratreeregressor = mean_error_iterative(df['HP Flare'], data_exr)\n",
        "error_decisiontree = mean_error_iterative(df['HP Flare'], data_dt)\n",
        "error_bayesian = mean_error_iterative(df['HP Flare'], data_bayesian)\n",
        "\n",
        "print( 'ITERATIVE KNN:', error_iterativeknn.mean(), error_iterativeknn.std())\n",
        "print( 'SIMPLE KNN:', error_simpleknn.mean(), error_simpleknn.std())\n",
        "print( 'EXTRA TREE REGRESSOR:', error_extratreeregressor.mean(), error_extratreeregressor.std())\n",
        "print( 'DECISION TREE:', error_decisiontree.mean(), error_decisiontree.std())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ITERATIVE KNN: 13.861840000000003 409.86307558087543\n",
            "SIMPLE KNN: 15.237 442.57111843024717\n",
            "EXTRA TREE REGRESSOR: 1.988089999999999 31.541883771453787\n",
            "DECISION TREE: 6.686100000000001 164.30537762590117\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: np.asscalar(a) is deprecated since NumPy v1.16, use a.item() instead\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O-nGdterf1VN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Looking at the HP FLARE Variabel\n",
        "nan = pd.DataFrame(df_nan)\n",
        "res_knn = pd.DataFrame(data_knn)\n",
        "res_knn1 = pd.DataFrame(data_knn1)\n",
        "res_exr = pd.DataFrame(data_exr)\n",
        "res_dec = pd.DataFrame(data_dt)\n",
        "res_bayesian = pd.DataFrame(data_bayesian)\n",
        "res = pd.DataFrame()\n",
        "res['NaN'] = nan[69]\n",
        "res['Iterative KNN'] = res_knn[69]\n",
        "res['KNN SimpleImputer'] = res_knn1[69]\n",
        "res['Iterative Extra Tree Regressor'] = res_exr[69]\n",
        "res['Iterative Decision Tree'] = res_dec[69]\n",
        "res['Iterative Bayesian Ridge'] = res_bayesian[69]\n",
        "\n",
        "res.to_excel(\"Multivariate_HPFLARE.xlsx\") \n",
        "df['HP Flare'].to_excel('Original.xlsx')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mrMWBmLOZh_i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Batch Generator for the RNN Network\n",
        "def batch_generator(batch_size, sequence_length, num_x_signals, num_y_signals, num_train, x_train_scaled, y_train_scaled):\n",
        "   \n",
        "    while True:\n",
        "        x_shape = (batch_size, sequence_length, num_x_signals)\n",
        "        x_batch = np.zeros(shape=x_shape, dtype=np.float16)\n",
        "\n",
        "        y_shape = (batch_size, sequence_length, num_y_signals)\n",
        "        y_batch = np.zeros(shape=y_shape, dtype=np.float16)\n",
        "\n",
        "        for i in range(batch_size):\n",
        "            idx = np.random.randint(num_train - sequence_length)\n",
        "            \n",
        "            x_batch[i] = x_train_scaled[idx:idx+sequence_length]\n",
        "            y_batch[i] = y_train_scaled[idx:idx+sequence_length]\n",
        "        \n",
        "        yield (x_batch, y_batch)\n",
        "\n",
        "\n",
        "warmup_steps = 50\n",
        "def loss_mse_warmup(y_true, y_pred):\n",
        "    y_true_slice = y_true[:, warmup_steps:, :]\n",
        "    y_pred_slice = y_pred[:, warmup_steps:, :]\n",
        "    mse = mean(square(y_true_slice - y_pred_slice))\n",
        "    \n",
        "    return mse\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKExuuksh0at",
        "colab_type": "text"
      },
      "source": [
        "Tentative Model for Validation the Imputation Techniques"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOhbME8Ihp_s",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "split = [0.5,0.6,0.7,0.8,0.9]\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "#split = [0.8]\n",
        "def run_model(data, df_targets):\n",
        "  #final_mse = np.empty((len(split)))\n",
        "  final_mse = []\n",
        "  for temp_split in split:\n",
        "    x_data = data.values\n",
        "    y_data = df_targets.values.reshape(-1,1)\n",
        "\n",
        "    num_data = len(x_data)\n",
        "    train_split = temp_split\n",
        "    num_train = int(train_split * num_data)\n",
        "    num_test = num_data - num_train\n",
        "\n",
        "    x_train = x_data[0:num_train]\n",
        "    x_test = x_data[num_train:]\n",
        "    y_train = y_data[0:num_train]\n",
        "    y_test = y_data[num_train:]\n",
        "\n",
        "    num_x_signals = x_data.shape[1]\n",
        "    num_y_signals = y_data.shape[1]\n",
        "\n",
        "    x_scaler = MinMaxScaler()\n",
        "    x_train_scaled = x_scaler.fit_transform(x_train)\n",
        "    x_test_scaled = x_scaler.transform(x_test)\n",
        "\n",
        "    y_scaler = MinMaxScaler()\n",
        "    y_train_scaled = y_scaler.fit_transform(y_train)\n",
        "    y_test_scaled = y_scaler.transform(y_test)\n",
        "\n",
        "    batch_size = 256\n",
        "    sequence_length = 100\n",
        "\n",
        "    generator = batch_generator(batch_size, sequence_length, num_x_signals, num_y_signals, num_train, x_train_scaled, y_train_scaled)\n",
        "\n",
        "    validation_data = (np.expand_dims(x_test_scaled, axis=0), np.expand_dims(y_test_scaled, axis=0))\n",
        "\n",
        "    #model_mse = np.empty((2))\n",
        "    model_mse = []\n",
        "    for i in range(2):\n",
        "      model = Sequential()\n",
        "      model.add(GRU(units=512,\n",
        "                    return_sequences=True,\n",
        "                    input_shape=(None, num_x_signals,)))\n",
        "      model.add(Dense(num_y_signals, activation='sigmoid'))\n",
        "\n",
        "      if False:\n",
        "          from tensorflow.python.keras.initializers import RandomUniform\n",
        "\n",
        "          # Maybe use lower init-ranges.\n",
        "          init = RandomUniform(minval=-0.05, maxval=0.05)\n",
        "\n",
        "          model.add(Dense(num_y_signals,\n",
        "                        activation='linear',\n",
        "                          kernel_initializer=init))\n",
        "\n",
        "      optimizer = RMSprop(lr=1e-3)\n",
        "      model.compile(loss=loss_mse_warmup, optimizer=optimizer, metrics = ['mse'])\n",
        "\n",
        "      path_checkpoint = 'best_model'\n",
        "      callback_checkpoint = ModelCheckpoint(filepath=path_checkpoint, monitor='val_loss', verbose=1, save_weights_only=True, save_best_only=True)\n",
        "      callback_early_stopping = EarlyStopping(monitor='val_loss', patience=3, verbose=1)\n",
        "      callbacks = [callback_checkpoint, callback_early_stopping]\n",
        "\n",
        "    \n",
        "      model.fit(x=generator,\n",
        "                epochs=10,\n",
        "                steps_per_epoch=100,\n",
        "                validation_data=validation_data, \n",
        "                callbacks = callbacks)\n",
        "      try:\n",
        "        model.load_weights(path_checkpoint)\n",
        "        print('Success')\n",
        "      except Exception as error:\n",
        "        print(\"Error trying to load checkpoint.\")\n",
        "        print(error)\n",
        "\n",
        "      # Input-signals for the model.\n",
        "      x = np.expand_dims(x_test_scaled, axis=0)\n",
        "\n",
        "      # Use the model to predict the output-signals.\n",
        "      y_pred = model.predict(x)\n",
        "      y_pred_rescaled = y_scaler.inverse_transform(y_pred[0])\n",
        "\n",
        "      temp_mse = np.sqrt(mean_squared_error(y_test, y_pred_rescaled)) \n",
        "      temp_mse = temp_mse.item()\n",
        "      #print(temp_mse)\n",
        "      model_mse.append(temp_mse)\n",
        "      #model_mse = np.append(model_mse, temp_mse)\n",
        "      #np.insert(model_mse,0, temp_mse)\n",
        "      print('model finished')\n",
        "\n",
        "\n",
        "    print('split finished')\n",
        "    #print(model_mse)\n",
        "    #print(np.mean(model_mse))\n",
        "    final_mse.append(np.mean(temp_mse))\n",
        "    #final_mse = np.append(final_mse, np.mean(model_mse))\n",
        "    #final_mse.insert(0, np.mean(model_mse))\n",
        "    #np.insert(final_mse, 0, model_mse)\n",
        "\n",
        "  return_final_mse = np.array(final_mse)\n",
        "\n",
        "\n",
        "\n",
        "  return return_final_mse\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KcTkaYhziOAP",
        "colab_type": "text"
      },
      "source": [
        "First Imputing the missing values, later predict the value of HP Flare with the use of a RNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CovSeh5wh-dz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imputation with Linear Interpolation\n",
        "data_interpolation = new.copy()\n",
        "data_interpolation = data_interpolation.interpolate(method = \"linear\", axis = 0).ffill().bfill()\n",
        "interpolation_score = run_model(data_interpolation, df_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w0o9WD9biAN6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imputation with Fill Forward\n",
        "data_ff = new.copy()\n",
        "data_ff = data_ff.fillna(method = \"ffill\")\n",
        "data_ff = data_ff.fillna(method = \"bfill\")\n",
        "ff_score = run_model(data_ff, df_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oLgN_-vriACy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imputation with BackWard Fill\n",
        "data_bf = new.copy()\n",
        "data_bf = data_bf.fillna(method = \"bfill\")\n",
        "data_bf = data_bf.fillna(method = \"ffill\")\n",
        "bf_score = run_model(data_bf, df_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NP20_6vIh_4C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imputation with Simple KNN Imputer\n",
        "data_knn = new.copy()\n",
        "imputer_knn = KNNImputer()\n",
        "imputer_knn.fit(data_knn)\n",
        "data_knn = imputer_knn.transform(data_knn)\n",
        "data_knn = pd.DataFrame(data_knn)\n",
        "result_knn = run_model(data_knn, df_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Alnn3ypvh_qC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Imputation with KNN Iterative Imputer\n",
        "data_knn1 = new.copy()\n",
        "knniterative_imputer = IterativeImputer(estimator = KNeighborsRegressor(n_neighbors = 10), random_state = 0, max_iter = 1000, tol = 0.08)\n",
        "knniterative_imputer = knniterative_imputer.fit(data_knn1)\n",
        "data_knn1 = knniterative_imputer.transform(data_knn1)\n",
        "data_knn1 = pd.DataFrame(data_knn1)\n",
        "result_iterativeknn = run_model(data_knn1, df_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uzmY35HUiWwr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Iterative DecisionTree()\n",
        "data_decision = new.copy()\n",
        "decisiontree_imputer = IterativeImputer(estimator = DecisionTreeRegressor(max_features = 'sqrt', random_state = 0), random_state = 0, max_iter = 1000, tol = 0.1)\n",
        "decisiontree_imputer = decisiontree_imputer.fit(data_decision)\n",
        "data_decision = decisiontree_imputer.transform(data_decision)\n",
        "data_decision = pd.DataFrame(data_decision)\n",
        "result_decison = run_model(data_decision, df_targets)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bX-MCScGiZIF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "results = ( (interpolation_score.mean(), interpolation_score.std()), \n",
        "           (ff_score.mean(), ff_score.std()), (bf_score.mean(), bf_score.std()), (result_knn.mean(), result_knn.std()), \n",
        "           (result_iterativeknn.mean(), result_iterativeknn.std()), \n",
        "            (result_decison.mean(), result_decison.std()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sUiZk24Ey-Rc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_function(output):\n",
        "  results = np.array(output[0])\n",
        "  names = output[1]\n",
        "  mse = results[:,0]\n",
        "  std = results[:,1]\n",
        "\n",
        "  bars = len(mse)\n",
        "  xval  = np.arange(bars)\n",
        "\n",
        "  colors = ['Lightblue']\n",
        "  plt.figure(figsize=(12,6))\n",
        "  ax1 = plt.subplot(121)\n",
        "\n",
        "  for i in xval:\n",
        "    ax1.barh(i, mse[i], xerr = std[i], color = 'Lightblue', alpha = 0.6, align = 'center')\n",
        "    ax1.set_xlim(left = np.min(mse)*0.8, right = np.max(mse)*1.2)\n",
        "    ax1.set_yticks(xval)\n",
        "    ax1.set_xlabel('Root Mean Squared Error w/ Standard Deviation')\n",
        "    ax1.invert_yaxis()\n",
        "    ax1.set_yticklabels(names)\n",
        "\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}